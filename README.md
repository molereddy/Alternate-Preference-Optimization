# Molereddy's TOFU: Task of Fictitious Unlearning üç¢

# COMPSCI 696 UMass Industry Research Project:
Changes to implement our unlearning ideas and made fixes and adapt the TOFU codebase to running on Unity.

---------------

The TOFU dataset serves as a benchmark for evaluating unlearning performance of large language models on realistic tasks. The dataset comprises question-answer pairs based on autobiographies of 200 different authors that do not exist and are completely fictitiously generated by the GPT-4 model. The goal of the task is to unlearn a fine-tuned model on various fractions of the forget set.

We notice that Llama2 model has reproducibility issue due to the internal randomness of flash attention. You are encouraged to collect your own retain results. Our huggingface leaderboard results and the numbers/figures in the paper are also subject to update. Feel free to contact us if you run into any issue! 

## Applicability üöÄ

The dataset is in QA format, making it ideal for use with popular chat models such as Llama2, Mistral, or Qwen. However, it also works for any other large language model. The corresponding code base is written for the Llama2 chat, and Phi-1.5 models, but can be easily adapted to other models.

## Installation (Unity)
```
module load conda/latest
conda create -n tofu python=3.10
conda activate tofu
module load cuda/12.6
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```

## Installation (Others)
```
conda create -n tofu python=3.10
conda activate tofu
pip install -r requirements.txt
pip install flash-attn --no-build-isolation
```
